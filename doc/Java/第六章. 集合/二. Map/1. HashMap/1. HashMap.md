# 1. HashMap

---

## Java 核心集合类：`java.util.HashMap` 深度解析 🗺️

### 1. 概述与定义 📜

`java.util.HashMap` 是 Java 集合框架（Java Collections Framework, JCF）中 `Map` 接口最常用的实现类之一。它提供了一种存储键值对（Key-Value Pair）映射关系的数据结构。其核心是基于 **哈希表（Hash Table）** 实现的，这使得它在理想情况下能够提供常数时间复杂度 O(1) 的插入、删除和查找操作。

**核心概念定义：**

- **键值对 (Key-Value Pair):** `HashMap` 存储的是成对的数据，每个数据项包含一个唯一的键（Key）和一个对应的值（Value）。通过键可以快速地找到对应的值。
- **基于哈希 (Hash-Based):** `HashMap` 内部通过计算键的哈希码 (`hashCode()`) 来决定其在内部存储结构（通常是一个数组，称为“桶”或“bucket”）中的位置。
- **允许 Null:** `HashMap` 允许 **一个** `null` 键和 **多个** `null` 值。`null` 键总是被映射到哈希表的特定位置（通常是索引 0）。
- **无序性 (Unordered):** `HashMap` 不保证元素的迭代顺序。元素的顺序可能会因为哈希冲突、扩容等原因在任何时候发生变化。你不应该依赖于 `HashMap` 的迭代顺序。如果需要有序的 Map，应考虑 `LinkedHashMap`（按插入或访问顺序）或 `TreeMap`（按自然顺序或自定义比较器顺序）。
- **非线程安全 (Not Thread-Safe):** `HashMap` 的实现不是同步的。如果在多线程环境中并发地修改（添加、删除元素，或者结构性修改如扩容）`HashMap`，并且没有外部同步措施，可能会导致数据不一致，甚至在 JDK 7 及更早版本中可能因扩容操作引发死循环。

`HashMap` 是 Java 开发中用于快速查找和关联数据的基石，理解其内部工作原理对于编写高效且健壮的代码至关重要。

### 2. 主要特点 ✨

`HashMap` 之所以被广泛使用，主要归功于其以下显著特点：

1. **高效的性能** 🚀:
   - **平均 O(1) 复杂度:** 对于 `put(K key, V value)` 和 `get(Object key)` 操作，在哈希分布均匀的情况下，平均时间复杂度为 O(1)。这是其最核心的优势。
   - **最坏 O(n) 复杂度:** 在极端情况（例如，所有键的哈希码都相同，导致所有元素集中在一个桶中形成长链表或复杂的树结构），时间复杂度会退化到 O(n)，其中 n 是 Map 中的元素数量。JDK 8 引入红黑树优化了这种情况下的性能，使其最坏复杂度降为 O(log n)。
2. \*\*依赖 ****`hashCode()`**** 和 \*\***`equals()`** 🔑🤝:
   - `HashMap` 的正确运行严重依赖于作为键的对象的 `hashCode()` 和 `equals()` 方法的正确实现。
   - `hashCode()` 用于快速定位桶的位置。
   - `equals()` 用于在发生哈希冲突时（即多个不同的键映射到同一个桶），精确地识别和区分键。
   - **重要契约:**
     - 如果 `a.equals(b)` 为 `true`，那么 `a.hashCode()` 必须等于 `b.hashCode()`。
     - 如果 `a.equals(b)` 为 `false`，`a.hashCode()` 和 `b.hashCode()` 可以相同（哈希冲突），也可以不同。
     - 一个对象的 `hashCode()` 值在其生命周期内，只要其 `equals()` 方法的比较信息没有改变，就应该保持不变。
   - 不遵守这个契约会导致 `HashMap` 无法正常工作（例如，`put` 进去的对象 `get` 不出来）。
3. **动态扩容 (Resizing)** 📈:
   - 当 `HashMap` 中的元素数量超过 **阈值 (threshold = capacity \* load factor)** 时，会自动进行扩容（通常是将容量翻倍），并将所有现有元素重新计算哈希位置（rehash）到新的、更大的数组中。这有助于维持较低的哈希冲突率，保证性能。
4. **空间换时间** 💰<->⏱️:
   - 哈希表是一种典型的空间换时间的策略。它使用额外的内存空间（内部数组）来加速查找过程。
5. **灵活性 (Null Support):** 如前所述，支持 `null` 键和 `null` 值，提供了使用的便利性，但在某些场景下也可能引入 `NullPointerException` 的风险，需要开发者注意。

### 3. 应用目标 (Use Cases) 🎯

`HashMap` 的设计目标是提供一个高效的、通用的键值对存储机制。它适用于以下典型场景：

1. **数据缓存 (Caching):** 快速存储和检索频繁访问的数据，如用户信息、配置项等。内存缓存是 `HashMap` 非常常见的应用。

   ```java
   // 简单内存缓存示例
   Map<String, UserData> userCache = new HashMap<>();
   UserData user = getUserFromDatabase("userId123");
   userCache.put("userId123", user);
   // ... later
   UserData cachedUser = userCache.get("userId123");
   if (cachedUser != null) {
       // Use cached data
   } else {
       // Fetch from database and cache
   }
   ```
2. **配置管理 (Configuration Management):** 存储和查找应用程序的配置参数。

   ```java
   Map<String, String> appConfig = new HashMap<>();
   appConfig.put("database.url", "jdbc:mysql://localhost:3306/mydb");
   appConfig.put("service.timeout", "5000");
   String dbUrl = appConfig.get("database.url");
   ```
3. **频率统计 (Frequency Counting):** 统计集合中各元素出现的次数。

   ```java
   String text = "hello world hello java";
   Map<String, Integer> wordCounts = new HashMap<>();
   for (String word : text.split(" ")) {
       wordCounts.put(word, wordCounts.getOrDefault(word, 0) + 1);
   }
   // wordCounts will be: {"hello": 2, "world": 1, "java": 1}
   ```
4. **数据索引 (Indexing):** 根据某个属性（作为 Key）快速查找对象（作为 Value）。

   ```java
   List<Product> productList = getAllProducts();
   Map<String, Product> productIndexById = new HashMap<>();
   for (Product p : productList) {
       productIndexById.put(p.getId(), p);
   }
   Product specificProduct = productIndexById.get("prod789");
   ```
5. **传递复杂参数:** 在方法间或模块间传递一组命名的参数。

总之，任何需要通过一个唯一的标识符（键）来快速存储、查找、更新或删除关联信息（值）的场景，`HashMap` 都是首选的数据结构之一。

### 4. 主要内容及其组成部分 (Internal Structure) 🏗️

理解 `HashMap` 的内部结构是掌握其工作原理的关键。其核心组成部分包括（以 JDK 8+ 为例）：

1. \*\*桶数组 (Bucket Array): \*\***`transient Node<K,V>[] table;`**

   - 这是 `HashMap` 的主体结构，一个 `Node` 类型的数组。每个数组元素称为一个“桶”（Bucket）。
   - 数组的长度（Capacity）**总是 2 的幂次方** (e.g., 16, 32, 64...)。这是为了使用高效的位运算 `(n - 1) & hash` 来代替取模运算 `%` 计算索引，提高性能。
   - 数组是 `transient` 的，意味着它不会被默认的序列化机制序列化。`HashMap` 有自定义的 `writeObject` 和 `readObject` 方法来处理序列化。
2. \*\*节点 (Node): \*\***`static class Node<K,V> implements Map.Entry<K,V>`**

   - 这是存储键值对的基本单元，实现了 `Map.Entry` 接口。
   - 每个 `Node` 对象包含：
     - `final int hash;`: 键的哈希值（经过 `HashMap` 内部 `hash()` 方法处理）。存储哈希值是为了在查找和扩容时避免重复计算。
     - `final K key;`: 键对象。
     - `V value;`: 值对象。
     - `Node<K,V> next;`: 指向同一个桶中的下一个节点的引用。这用于解决哈希冲突，形成 **链表**。
3. \*\*红黑树节点 (TreeNode): \*\***`static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V>`** (JDK 8+)

   - 当某个桶中的链表长度达到一定阈值 (`TREEIFY_THRESHOLD`)，并且 `HashMap` 的总容量也达到一定阈值 (`MIN_TREEIFY_CAPACITY`) 时，这个链表会被转换成 **红黑树 (Red-Black Tree)** 结构。
   - `TreeNode` 继承自 `LinkedHashMap.Entry`（它本身继承自 `HashMap.Node`），并添加了红黑树所需的额外属性，如 `parent`, `left`, `right`, `red` (颜色标记)。
   - 使用红黑树的目的是将该桶内的查找、插入、删除操作的时间复杂度从 O(n) 优化到 O(log n)，提高在哈希冲突严重时的性能。
4. **重要常量与变量:**

   - **`size`**: `HashMap` 中实际存储的键值对数量。
   - **`modCount`**: 修改计数器。用于迭代器实现快速失败 (Fail-Fast) 机制。每次结构性修改（增、删、扩容）时，`modCount` 都会增加。迭代器在迭代过程中会检查 `modCount` 是否与创建迭代器时的值一致，不一致则抛出 `ConcurrentModificationException`。
   - **`threshold`**: 扩容阈值。当 `size` 超过 `threshold` 时，触发 `resize()` 操作。`threshold = capacity * loadFactor`。
   - **`loadFactor`**: 负载因子。表示哈希表的填充程度。
     **核心常量解释表:**
     名                  | 默认值 | 含义                                                                                                                                 |
     \| :---------------------- | :----- | :----------------------------------------------------------------------------------------------------------------------------------- |
     \| `DEFAULT_INITIAL_CAPACITY` | 16     | 默认初始容量（桶数组大小）。若未指定，则首次 `put` 时创建大小为 16 的数组。                                                               |
     \| `MAXIMUM_CAPACITY`      | 2^30   | 最大容量。即使指定或计算出的容量超过此值，也会被限制为此值。                                                                              |
     \| `DEFAULT_LOAD_FACTOR`   | 0.75f  | 默认负载因子。这是时间和空间成本之间的一个平衡选择。值越小，冲突越少，但空间利用率低；值越大，空间利用率高，但冲突增多，性能可能下降。 |
     \| `TREEIFY_THRESHOLD`     | 8      | 链表转红黑树的阈值。当一个桶中的节点数达到此值时，**可能**会触发树化。                                                                    |
     \| `UNTREEIFY_THRESHOLD`   | 6      | 红黑树转链表的阈值。在 `resize` 操作中，如果树节点数减少到此值，会转回链表结构。                                                           |
     \| `MIN_TREEIFY_CAPACITY`  | 64     | 触发树化的最小哈希表容量。即使链表长度达到 `TREEIFY_THRESHOLD`，如果当前 `table` 的容量小于此值，则优先进行扩容 (`resize`) 而不是树化。       |

   **HashMap 内部结构示意图 (Mermaid):**

   ```mermaid
   graph TD
       subgraph HashMap Internal Structure
           A[table: Node[]] --> B(Bucket 0);
           A --> C(Bucket 1);
           A --> D(Bucket ...);
           A --> E(Bucket i);
           A --> F(Bucket j);
           A --> G(Bucket n-1);

           subgraph Bucket i (LinkedList Collision)
               E --> N1[Node(k1, v1, h1)];
               N1 --> N2[Node(k2, v2, h2)];
               N2 --> N3[Node(k3, v3, h3)];
           end

           subgraph Bucket j (Red-Black Tree Collision)
               F --> T1(TreeNode);
               T1 -- R --> T2(TreeNode);
               T1 -- L --> T3(TreeNode);
               T2 -- L --> T4(TreeNode);
               T2 -- R --> T5(TreeNode);
               T3 -- R --> T6(TreeNode);
               %% R = Right child, L = Left child
           end

           C(Bucket 1) --> N4[Node(k4, v4, h4)];
           G(Bucket n-1) --> N5[Node(k5, v5, h5)];
           B(Bucket 0) --- N_null[Node(null, v_null, 0)]; %% Null key example
       end

       style T1 fill:#f9f,stroke:#333,stroke-width:2px %% Style TreeNode
       style T2 fill:#f9f,stroke:#333,stroke-width:2px
       style T3 fill:#f9f,stroke:#333,stroke-width:2px
       style T4 fill:#f9f,stroke:#333,stroke-width:2px
       style T5 fill:#f9f,stroke:#333,stroke-width:2px
       style T6 fill:#f9f,stroke:#333,stroke-width:2px
   ```

   **图表解释:** 上图展示了 `HashMap` 内部的 `table` 数组（桶数组）。每个桶可能为空，或者包含一个 `Node`。如果发生哈希冲突，多个 `Node` 会通过 `next` 指针形成链表（如 Bucket i）。当链表过长（且满足条件）时，会转换为红黑树结构（如 Bucket j）。`null` 键通常存储在 Bucket 0。

### 5. 原理剖析 (深入解析, 源码级) ⚙️🔬

理解 `HashMap` 的核心操作原理是面试中的重中之重。

#### **哈希函数 (****`hash(Object key)`****)**

`HashMap` 并不是直接使用键的 `hashCode()` 返回值。它内部有一个 `hash()` 方法对键的 `hashCode()` 进行再处理：

```java
static final int hash(Object key) {
    int h;
    // 如果 key 是 null，返回 0。
    // 否则，获取 key 的 hashCode()，然后将 hashCode 的高 16 位与低 16 位进行异或 (^) 运算。
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

**目的：** 这样做是为了**扰动计算**，让哈希值的高位也能参与到最终索引的计算中，减少哈希冲突的可能性。因为计算索引时通常只用了低位（` (n - 1) & hash` ），如果不进行扰动，即使 `hashCode()` 实现得很好，如果高位变化很大而低位变化小，也容易造成冲突。

#### **计算索引 (****`indexFor(hash, length)`**** 或 ****`(n - 1) & hash`****)**

确定键值对在 `table` 数组中的位置（索引）使用位运算：

```java
// n 是 table 的长度 (capacity)，必须是 2 的幂次方
int index = (n - 1) & hash;
```

**原理：** 当 `n` 是 2 的幂次方时，`n - 1` 的二进制表示就是一串连续的 1（例如，`n=16`, `n-1=15`, 二进制 `1111`）。`hash & (n - 1)` 的效果等同于 `hash % n`，但是位运算 `&` 比取模运算 `%` 效率高得多。这就是为什么 `HashMap` 容量强制为 2 的幂次方。

#### **`put(K key, V value)`**\*\* 方法详解\*\*

`put` 操作是 `HashMap` 最核心的操作之一，其流程（JDK 8+）大致如下：

1. **判空与哈希:**
   - 如果 `table` 未初始化或长度为 0，调用 `resize()` 进行初始化（通常是默认容量 16）。
   - 计算键 `key` 的哈希值 `hash = hash(key)`。
   - 计算索引 `i = (table.length - 1) & hash`。
2. **检查桶位:**
   - 获取 `table[i]` 处的节点 `p`。
   - **如果 ****`p`**** 为 ****`null`****:** 说明该桶为空，直接创建一个新的 `Node(hash, key, value, null)` 放入 `table[i]`。
3. **处理冲突 (桶位非空):**
   - **如果 ****`p`**** 的哈希值等于 ****`hash`**** 且 ****`p`**** 的键等于 ****`key`**** (通过 ****`equals()`**** 判断):** 说明找到了相同的键，用新值 `value` 覆盖旧值。将旧值存储在 `e` 中待返回。
   - **如果 ****`p`**** 是 ****`TreeNode`****:** 调用红黑树的插入方法 `p.putTreeVal(this, table, hash, key, value)`。树结构内部会处理键重复或插入新节点的情况。
   - **如果 ****`p`**** 是普通 ****`Node`**** (链表头):** 遍历链表：
     - 使用 `for` 循环和计数器 `binCount` 遍历链表节点。
     - 对于每个节点 `e`，检查其 `hash` 和 `key` 是否与待插入的 `key` 相同。如果相同，则找到重复键，用新 `value` 替换旧 `value`，记录旧值，并跳出循环。
     - 如果遍历到链表末尾（`e.next == null`）仍未找到相同键，则在链表尾部插入新的 `Node(hash, key, value, null)`。
     - **树化检查:** 在插入新节点**之后**，检查链表长度 `binCount` 是否达到 `TREEIFY_THRESHOLD - 1`（即插入后长度为 8）。如果是，并且 `table.length >= MIN_TREEIFY_CAPACITY`，则调用 `treeifyBin(table, hash)` 将此链表转换为红黑树。
     - 如果找到重复键并更新了值，则跳出链表遍历。
4. **收尾工作:**
   - 如果 `put` 操作确实插入了一个新的键值对（而不是更新现有键的值），则：
     - 递增 `modCount`。
     - 递增 `size`。
     - 检查 `size` 是否超过 `threshold`。如果是，则调用 `resize()` 进行扩容。
   - `afterNodeInsertion(evict)`: 这是一个空方法，留给子类（如 `LinkedHashMap`）实现插入后逻辑。
   - 返回旧值（如果发生替换）或 `null`（如果是新插入）。

**`put`**\*\* 流程图 (Mermaid - Simplified):\*\* 

```mermaid
graph TD
    A[Start put(key, value)] --> B{table null?};
    B -- Yes --> C[resize() to initialize];
    B -- No --> D[Calculate hash & index i];
    C --> D;
    D --> E{table[i] == null?};
    E -- Yes --> F[Create Node, put in table[i]];
    E -- No --> G{First node matches key?};
    G -- Yes --> H[Update value, store old];
    G -- No --> I{First node TreeNode?};
    I -- Yes --> J[Call putTreeVal()];
    I -- No --> K[Traverse LinkedList];
    K --> L{Found matching key?};
    L -- Yes --> H;
    L -- No --> M[Append new Node to list tail];
    M --> N{List length >= TREEIFY_THRESHOLD - 1?};
    N -- Yes --> O{table.length >= MIN_TREEIFY_CAPACITY?};
    O -- Yes --> P[treeifyBin()];
    O -- No --> Q;
    N -- No --> Q;
    P --> Q;
    J --> Q;
    H --> R;
    F --> Q[Increment size, modCount];
    Q --> S{size > threshold?};
    S -- Yes --> T[resize()];
    S -- No --> R[Return old value or null];
    T --> R;
```

**图表解释:** 该流程图简化了 `put` 方法的主要逻辑：初始化检查、计算位置、空桶处理、冲突处理（检查首节点、树节点、遍历链表）、插入新节点、可能的树化、更新计数器、检查扩容，最后返回。

#### **`get(Object key)`**\*\* 方法详解\*\*

`get` 操作相对简单：

1. **计算哈希与索引:** 计算 `key` 的 `hash` 和 `table` 中的索引 `i`。
2. **查找桶:** 获取 `table[i]` 处的节点 `first`。
3. **检查首节点:** 如果 `first` 不为 `null`：
   - 检查 `first.hash` 是否等于 `hash` 并且 `first.key` 是否等于 `key` (通过 `equals()` 判断)。如果匹配，返回 `first.value`。
4. **查找后续节点:** 如果首节点不匹配且 `first.next` 不为 `null`：
   - **如果是 ****`TreeNode`****:** 调用 `first.getTreeNode(hash, key)` 在红黑树中查找。
   - **如果是链表:** 遍历链表（`do...while` 循环），对每个节点 `e`，检查 `hash` 和 `key` 是否匹配。找到则返回 `e.value`。
5. **未找到:** 如果遍历完桶（或桶为空）仍未找到匹配的键，返回 `null`。

#### **`resize()`**\*\* 方法详解\*\*

扩容是 `HashMap` 维护性能的关键，也是相对复杂的操作（尤其在 JDK 8+ 中有优化）：

1. **容量计算:**
   - 获取旧容量 `oldCap` 和旧阈值 `oldThr`。
   - 计算新容量 `newCap` (通常是 `oldCap << 1`，即翻倍) 和新阈值 `newThr` (通常是 `newCap * loadFactor`)。需要处理边界情况（如初始容量、超过最大容量）。
   - 如果 `oldCap` 已经达到 `MAXIMUM_CAPACITY`，则不再扩容，只调整 `threshold` 为 `Integer.MAX_VALUE`。
2. **创建新表:** 创建一个新的 `Node` 数组 `newTab`，长度为 `newCap`。
3. **迁移元素:** 遍历旧表 `oldTab` 的每一个桶（`for` 循环 `j` from 0 to `oldCap`）：
   - 获取旧桶的头节点 `e = oldTab[j]`。
   - 如果 `e` 不为 `null`：
     - 将 `oldTab[j]` 设为 `null`（帮助 GC）。
     - **如果 ****`e.next == null`****:** 该桶只有一个节点。直接计算它在新表中的索引 `(newCap - 1) & e.hash`，并放入 `newTab`。
     - **如果 ****`e`**** 是 ****`TreeNode`****:** 调用 `((TreeNode<K,V>)e).split(this, newTab, j, oldCap)`。这个方法利用红黑树的性质，高效地将树拆分成两个子树或链表，分别放到新表中的 `j` 和 `j + oldCap` 位置。
     - **如果是链表 (JDK 8+ 优化):**
       - 维护两个链表的头尾指针：`loHead`, `loTail` (用于索引 `j`) 和 `hiHead`, `hiTail` (用于索引 `j + oldCap`)。
       - 遍历旧链表中的每个节点 `e`：
         - **判断依据:** `(e.hash & oldCap) == 0`。这个位运算巧妙地利用了新容量是旧容量两倍的特性。如果结果为 0，节点在新表中的索引仍然是 `j`；如果结果非 0，索引将是 `j + oldCap`。
         - 根据判断结果，将节点追加到 `lo` 链表或 `hi` 链表的尾部。
       - 遍历结束后，如果 `loHead` 不为空，将 `lo` 链表放入 `newTab[j]`。
       - 如果 `hiHead` 不为空，将 `hi` 链表放入 `newTab[j + oldCap]`。
       - 这个优化避免了对链表中每个节点重新计算完整的哈希索引，只需一次位运算即可确定其在新表中的两个可能位置之一。
4. **更新引用:** 将 `HashMap` 的 `table` 字段指向 `newTab`，更新 `threshold`。

**`resize()`**\*\* 中 JDK 8 的链表迁移优化示意图 (Mermaid):\*\* 

```mermaid
graph TD
    subgraph Old Table (Capacity = 16)
        Bucket_j[Bucket j] --> N1((e: Node 1));
        N1 --> N2((Node 2));
        N2 --> N3((Node 3));
        N3 --> N4((Node 4));
    end

    subgraph New Table (Capacity = 32)
        NewBucket_j[Bucket j];
        NewBucket_j_plus_oldCap[Bucket j + 16];
    end

    subgraph Processing Bucket j
        P1{Loop: Node 1 (e)};
        P1 -- (e.hash & 16) == 0 --> AddToLowList[Add N1 to Low List (index j)];
        P1 -- (e.hash & 16) != 0 --> AddToHighList[Add N1 to High List (index j+16)];

        P2{Loop: Node 2 (e)};
        P2 -- (e.hash & 16) == 0 --> AddToLowList;
        P2 -- (e.hash & 16) != 0 --> AddToHighList;

        P3{Loop: Node 3 (e)};
        P3 -- (e.hash & 16) == 0 --> AddToLowList;
        P3 -- (e.hash & 16) != 0 --> AddToHighList;

        P4{Loop: Node 4 (e)};
        P4 -- (e.hash & 16) == 0 --> AddToLowList;
        P4 -- (e.hash & 16) != 0 --> AddToHighList;
    end

    AddToLowList --> FinalLowList[Low List Head --> ... --> Low List Tail];
    AddToHighList --> FinalHighList[High List Head --> ... --> High List Tail];

    FinalLowList --> PlaceLow[Place Low List in NewBucket_j];
    FinalHighList --> PlaceHigh[Place High List in NewBucket_j_plus_oldCap];

    Bucket_j -.-> P1;
    N1 -.-> P2;
    N2 -.-> P3;
    N3 -.-> P4;

    style AddToLowList fill:#ccf
    style AddToHighList fill:#fcc
```

**图表解释:** 当从容量 16 扩容到 32 时，旧 Bucket `j` 中的链表节点会被遍历。通过检查 `(e.hash & 16)`（`16` 是旧容量 `oldCap`）是否为 0，将节点分配到两个新链表：Low List（保持在索引 `j`）和 High List（移动到索引 `j + 16`）。最后将这两个新链表分别放入新表对应位置。

#### **JDK 7 vs JDK 8+ 的主要差异**

- **冲突解决:** JDK 7 只使用链表。JDK 8+ 在链表长度达到阈值时会转换为红黑树。
- **`put`**\*\* 插入:\*\* JDK 7 使用头插法，JDK 8+ 使用尾插法。头插法在并发 `resize` 时可能导致链表成环，引发死循环。尾插法避免了这个问题。
- **`resize`**\*\* 迁移:\*\* JDK 7 需要重新计算每个元素在新表中的完整索引。JDK 8+ 对链表迁移做了优化（如上图所示），效率更高。

### 6. 应用与拓展 🚀

#### **最佳实践与注意事项**

1. **预估容量:** 如果能预估 `HashMap` 将存储的元素数量，在创建时指定 `initialCapacity` 可以避免或减少后续的 `resize` 操作，提高性能。计算合适的 `initialCapacity` 时，考虑负载因子：`initialCapacity = (int) (expectedSize / loadFactor) + 1`。
2. **自定义对象作 Key:** 如果使用自定义类的对象作为 `HashMap` 的键，**必须**正确地重写 `hashCode()` 和 `equals()` 方法，并保证它们遵守约定。
3. **避免使用可变对象作 Key:** 如果一个对象作为 Key 放入 `HashMap` 后，其内部状态发生改变，导致 `hashCode()` 值变化或 `equals()` 比较结果变化，那么可能无法再通过 `get()` 方法取回该对象。通常建议使用不可变对象（如 `String`, `Integer`, 或自定义的不可变类）作为 Key。
4. **理解 ****`null`****:** 注意 `HashMap` 对 `null` 键和值的支持，在处理时考虑 `NullPointerException` 的可能性。

#### **线程安全问题与替代方案**

- **`HashMap`**\*\* 是非线程安全的。\*\* 在多线程环境下：
  - 并发 `put` 可能导致数据丢失（某个线程的写入被覆盖）。
  - 并发 `put` 和 `get` 可能读到不一致的数据。
  - 并发 `resize` (尤其 JDK 7) 可能导致链表循环，使得 `get` 操作陷入死循环，CPU 飙升。
  - 在迭代过程中进行结构性修改（非迭代器自身的 `remove`），会触发 `ConcurrentModificationException` (Fail-Fast)。
- **线程安全的替代方案:**
  - **`Hashtable`:** 古老的线程安全 Map 实现，所有公开方法都使用 `synchronized` 修饰，锁粒度是整个对象。性能较差，因为它在任何操作时都会锁住整个表，并发度低。不允许 `null` 键和值。
  - **`Collections.synchronizedMap(Map<K,V> m)`:** 一个包装器方法，返回指定 Map 的线程安全视图。其内部也是通过 `synchronized` 锁住整个 Map 对象来实现同步。性能与 `Hashtable` 类似，并发度低。
  - **`ConcurrentHashMap`:** (推荐 👍) JUC 包下的类，专为高并发场景设计。它采用了更细粒度的锁机制（JDK 7 的分段锁 Segment，JDK 8+ 的 CAS + `synchronized` 锁住 Node 或 ForwardingNode），允许多个线程同时读写 Map 的不同部分，提供了高得多的并发性能。它是现代 Java 并发编程中替代 `HashMap` 和 `Hashtable` 的首选。

#### **有序 Map**

如果需要保持 Map 中元素的顺序，应使用：

- **`LinkedHashMap`:** 继承自 `HashMap`，额外维护了一个双向链表来记录元素的插入顺序或访问顺序。迭代时按此顺序返回。性能略低于 `HashMap`（因为维护链表的开销），但仍然是 O(1) 级别的。
- **`TreeMap`:** 基于红黑树实现。元素根据键的自然顺序（需要 Key 实现 `Comparable` 接口）或构造时提供的 `Comparator` 进行排序。提供了按排序顺序迭代的功能，以及一些基于排序的操作（如 `firstKey()`, `lastKey()`, `subMap()` 等）。查找、插入、删除操作的平均和最坏时间复杂度都是 O(log n)。

### 7. 面试问答 💬

**(注意：以下内容模拟面试者口吻回答)**

**面试官问 1：请详细介绍一下 HashMap 的内部工作原理，特别是 put 和 get 方法的过程。**

**面试者答：**

好的面试官。`HashMap` 是基于哈希表实现的，内部核心是一个 `Node` 类型的数组，称为 `table` 或桶数组。

**关于 ****`put(key, value)`**** 方法：**

1. 首先，它会判断 `table` 是否已初始化，如果没有，会先调用 `resize()` 创建一个默认容量（通常是 16）的数组。
2. 然后，它计算 `key` 的哈希值。`HashMap` 不直接用 `key.hashCode()`，而是用一个内部的 `hash()` 方法，将 `hashCode()` 的高 16 位与低 16 位进行异或，目的是让哈希值分布更均匀，减少冲突。
3. 接着，通过 `(n - 1) & hash`（`n` 是数组长度）计算出该键值对应在 `table` 数组中的索引位置 `i`。因为 `n` 总是 2 的幂次方，这个位运算等效于取模，但效率更高。
4. 查看 `table[i]` 位置：
   - 如果这个位置是空的 (`null`)，就直接创建一个新的 `Node` 节点（包含 hash, key, value, next=null）放在这里。
   - 如果这个位置非空，说明发生了哈希冲突。
     - 首先检查第一个节点的 `key` 是否和当前要插入的 `key` 相同（通过 `hash` 值和 `equals()` 方法判断）。如果相同，就用新的 `value` 替换旧的 `value`，并返回旧值。
     - 如果第一个节点不同，就要看这个桶的结构：
       - 如果是 JDK 8+，并且节点是 `TreeNode` 类型，就调用红黑树的插入逻辑 (`putTreeVal`)。
       - 如果是链表，就遍历这个链表。对链表中的每个节点，都比较 `key` 是否相同。如果找到相同的 `key`，就更新 `value` 并返回旧值。如果遍历到链表末尾还没找到，就在链表尾部插入新的 `Node`。
       - 在 JDK 8+ 中，向链表插入新节点后，会检查链表长度。如果长度达到 `TREEIFY_THRESHOLD`（默认是 8），并且当前 `HashMap` 的总容量 `table.length` 大于等于 `MIN_TREEIFY_CAPACITY`（默认是 64），就会把这个链表转换成红黑树，以优化后续操作的性能。
5. 如果成功插入了一个新的节点（而不是更新），`HashMap` 会增加 `size` 计数器。
6. 然后，它会检查 `size` 是否超过了 `threshold`（阈值 = 容量 \* 负载因子）。如果超过了，就会调用 `resize()` 方法进行扩容。
7. 最后，`put` 方法返回被替换掉的旧 `value`（如果发生更新），或者返回 `null`（如果是新插入）。

**关于 ****`get(key)`**** 方法：**

1. 同样先计算 `key` 的 `hash` 值和在 `table` 中的索引 `i`。
2. 找到 `table[i]` 位置的第一个节点。
3. 如果第一个节点存在，并且其 `key` 正好与要查找的 `key` 相同（通过 `hash` 和 `equals()` 判断），则直接返回该节点的 `value`。
4. 如果第一个节点不匹配，或者 `table[i]` 位置本来就是空的，就要看后续情况：
   - 如果第一个节点是 `TreeNode`，就调用红黑树的查找方法 (`getTreeNode`)。
   - 如果是链表，就遍历链表，对每个节点比较 `key` 是否相同，找到即返回对应的 `value`。
5. 如果在整个桶（无论是单个节点、链表还是红黑树）中都没有找到匹配的 `key`，`get` 方法最终返回 `null`。

总的来说，`HashMap` 利用哈希和索引快速定位，并通过链表或红黑树解决冲突，实现了高效的存取。

**面试官问 2：为什么 HashMap 的容量（Capacity）推荐是 2 的幂次方？**

**面试者答：**

`HashMap` 的容量设计为 2 的幂次方，主要是为了 **提高计算索引的效率** 和 **帮助减少哈希冲突**。

1. **高效的索引计算:** 当容量 `n` 是 2 的幂次方时，它的二进制表示形式是 1 后面跟着若干个 0（例如，16 是 `10000`）。那么 `n - 1` 的二进制就是一串连续的 1（例如，15 是 `01111`）。计算一个键应该落在哪个桶（索引）时，`HashMap` 使用的是 `index = hash & (n - 1)` 这个位运算。这个 `&` 操作非常高效，它实际上等价于 `hash % n`（取模运算），但位运算比取模运算在 CPU 层面快得多。如果容量不是 2 的幂次方，就无法使用这个高效的位运算来代替取模了。
2. **更均匀的分布:** 使用 `hash & (n - 1)` 这个方式计算索引，能够确保只要 `hash` 值本身分布相对均匀，那么计算出的索引也会在 `[0, n-1]` 这个范围内相对均匀地分布。这有助于减少哈希冲突，让元素更平均地分散到各个桶中，从而维持 `HashMap` 的 O(1) 平均性能。如果容量不是 2 的幂次方，使用取模运算时，某些索引位置被选中的概率可能会不均匀，增加冲突风险。

特别是在 `resize` 扩容（容量翻倍，仍然是 2 的幂次方）时，JDK 8 的优化也利用了这个特性：元素在新表中的位置要么在原索引 `j`，要么在 `j + oldCapacity`，判断依据是 `(e.hash & oldCapacity) == 0`，这也依赖于容量是 2 的幂次方。

所以，将容量设为 2 的幂次方，是 `HashMap` 在性能和实现复杂度之间做出的一个非常重要的设计决策。

**面试官问 3：能比较一下 HashMap、Hashtable 和 ConcurrentHashMap 吗？它们之间有什么区别？**

**面试者答：**

好的，`HashMap`、`Hashtable` 和 `ConcurrentHashMap` 都是 Java 中 `Map` 接口的实现，用于存储键值对，但它们在 **线程安全性、对 null 的支持、以及性能** 方面有显著区别：

1. **线程安全性:**
   - **`HashMap`:** 是 **非线程安全** 的。如果在多线程环境下不加同步地修改 `HashMap`，可能会导致数据不一致、`ConcurrentModificationException`，甚至在 JDK 7 及之前版本扩容时出现死循环。
   - **`Hashtable`:** 是 **线程安全** 的。它的几乎所有公开方法（如 `put`, `get`, `remove`, `size` 等）都使用了 `synchronized` 关键字修饰，锁住的是整个 `Hashtable` 对象 (`this`)。这意味着在同一时间只有一个线程能访问 `Hashtable` 的任何方法。
   - **`ConcurrentHashMap`:** 也是 **线程安全** 的，但它采用了更高级的并发技术来实现更高的并发度。
     - 在 JDK 7 中，它使用了 **分段锁 (Segment)** 的机制。整个 `ConcurrentHashMap` 内部由多个 `Segment` 组成，每个 `Segment` 类似一个小的 `Hashtable`，有自己的锁。不同 `Segment` 的操作可以并发进行，锁的粒度是 `Segment`。默认有 16 个 `Segment`。
     - 在 JDK 8 及以后版本中，它放弃了分段锁，改用了 \*\*CAS (Compare-And-Swap) + \*\***`synchronized`** 的方式。对桶的更新操作通常先尝试 CAS，如果失败（说明有竞争），则对桶的头节点（或特定节点，如 ForwardingNode）使用 `synchronized` 加锁。锁的粒度是桶的头节点，大大减小了锁的范围，并发性能更好。`size()` 等操作则通过 CAS 维护计数或动态计算。
2. **对 Null 的支持:**
   - **`HashMap`:** 允许 **一个 ****`null`**** 键** 和 **多个 ****`null`**** 值**。
   - **`Hashtable`:** **不允许** `null` 键和 `null` 值。如果尝试存入 `null`，会直接抛出 `NullPointerException`。
   - **`ConcurrentHashMap`:** **不允许** `null` 键和 `null` 值。这主要是因为在并发环境下，`get(key)` 返回 `null` 时，无法区分是“这个 key 确实没有映射”，还是“这个 key 对应的 value 恰好是 null”。为了避免这种歧义，`ConcurrentHashMap` 禁止存入 `null`。
3. **性能:**
   - **`HashMap`:** 在单线程环境下性能最好，因为它没有任何同步开销。
   - **`Hashtable`:** 性能最差，因为所有操作都需要竞争同一把锁，导致并发度极低，基本上是串行执行。
   - **`ConcurrentHashMap`:** 在高并发环境下性能最好。由于其细粒度的锁机制（或无锁操作），多个线程可以同时对 Map 的不同部分进行操作，提供了非常高的吞吐量。

**总结表格:**

| 特性           | \`HashMap\`              | \`Hashtable\`                     | \`ConcurrentHashMap\` (JDK 8+)  |
| -------------- | ------------------------ | --------------------------------- | ------------------------------- |
| 线程安全       | 否                       | 是 (锁整个对象)                   | 是 (CAS + synchronized 锁 Node) |
| Null Key       | 允许 (1个)               | 否 (抛 NPE)                       | 否 (抛 NPE)                     |
| Null Value     | 允许 (多个)              | 否 (抛 NPE)                       | 否 (抛 NPE)                     |
| 性能 (单线程)  | 最高                     | 最低                              | 较高 (有少量同步开销)           |
| 性能 (高并发)  | 不可用 (需外部同步)      | 最低                              | 最高                            |
| Fail-Fast 迭代 | 是                       | 否 (迭代器是强一致性, 但可能阻塞) | 否 (迭代器是弱一致性, 不抛 CME) |
| 继承/实现      | \`AbstractMap\`          | \`Dictionary\`, \`AbstractMap\`   | \`AbstractMap\`                 |
| 推荐使用场景   | 单线程环境，或需外部同步 | 不推荐使用 (遗留类)               | 高并发环境                      |

因此，在现代 Java 开发中，单线程优先考虑 `HashMap`，需要线程安全的并发场景则强烈推荐使用 `ConcurrentHashMap`。`Hashtable` 基本已被废弃。

**面试官问 4：当 HashMap 发生哈希冲突（Hash Collision）时，它是如何解决的？JDK 8 有哪些优化？**

**面试者答：**

哈希冲突是指两个不同的键（Key）经过哈希函数计算后得到了相同的索引值，因此需要被映射到 `HashMap` 内部数组的同一个位置（同一个桶）。`HashMap` 主要使用 **拉链法 (Chaining)** 来解决哈希冲突。

**基本的解决方式 (拉链法):**

当发生冲突时，`HashMap` 不会覆盖已有的元素，而是在该索引位置维护一个数据结构来存储所有映射到此处的键值对。

- **JDK 7 及之前:** 在冲突的桶位，`HashMap` 会维护一个 **单向链表**。新加入的元素（发生冲突的元素）会通过 **头插法** 添加到链表的头部。查找时，需要遍历这个链表，逐个使用 `equals()` 方法比较键，直到找到匹配的键或遍历完整个链表。
- **JDK 8 及之后:** JDK 8 对冲突解决做了重要优化：
  1. **仍然使用链表:** 初始冲突时，依然在该桶位形成一个链表。但插入方式改为了 **尾插法**。这主要是为了配合 `resize` 时的优化，并避免了 JDK 7 头插法在并发 `resize` 时可能产生的环形链表问题。
  2. **引入红黑树:** 当同一个桶中的链表长度达到一个阈值 **`TREEIFY_THRESHOLD`**\*\* (默认为 8)\*\* 时，并且 **同时** 当前 `HashMap` 的总容量 `table.length` 大于等于 **`MIN_TREEIFY_CAPACITY`**\*\* (默认为 64)\*\* 时，这个链表就会被转换成一个 **红黑树 (Red-Black Tree)**。
     - **为什么需要容量阈值?** 如果容量很小，即使链表长度达到 8，可能 resize 扩容是更好的选择（因为扩容后元素会重新散列，冲突自然减少）。只有当容量已经比较大时，转换成红黑树才有意义。
     - **红黑树的好处:** 红黑树是一种自平衡的二叉查找树，其查找、插入、删除操作的平均和最坏时间复杂度都是 O(log n)，n 是树中节点数。相比于链表的 O(n) 复杂度，当冲突非常严重时（例如，很多键的哈希值相同），红黑树能显著提高性能，避免 `HashMap` 在极端情况下的性能退化。
  3. **树的退化:** 在 `resize()` 操作中，如果一个桶原来是红黑树，但在迁移到新表后，该桶（或拆分后的桶）中的节点数量减少到了 **`UNTREEIFY_THRESHOLD`**\*\* (默认为 6)\*\* 以下，那么这棵红黑树会被退化（转换回）成链表结构。这是因为节点数少时，链表的操作开销（常数因子小）可能比红黑树更低，且结构更简单。

**总结 JDK 8 的优化:**

- **链表转红黑树:** 解决了极端哈希冲突下链表过长导致的性能下降问题，将最坏时间复杂度从 O(n) 优化到 O(log n)。
- **尾插法:** 解决了并发 `resize` 可能产生的死循环问题，并配合了 `resize` 优化。
- **`resize`**\*\* 优化:\*\* 迁移链表元素时，通过 `(e.hash & oldCap)` 判断，直接将节点分配到新表的两个可能位置之一，避免了对每个节点重新计算哈希索引，提高了扩容效率。

这些优化使得 JDK 8 及以后的 `HashMap` 在保持高性能的同时，也更加健壮。

**面试官问 5：如果我们使用自定义对象作为 HashMap 的 Key，需要注意什么？为什么？**

**面试者答：**

当使用自定义类的对象作为 `HashMap` 的 Key 时，最重要的一点是 **必须正确地重写该类的 ****`hashCode()`**** 和 ****`equals()`**** 方法，并严格遵守它们之间的约定（或称为契约）**。

**原因如下：**

1. **`hashCode()`**\*\* 用于定位:\*\* `HashMap` 在 `put` 和 `get` 操作时，首先会调用 Key 对象的 `hashCode()` 方法得到一个哈希码，然后用这个哈希码（经过内部 `hash()` 函数处理）来计算该 Key 在内部 `table` 数组中的索引位置。如果 `hashCode()` 方法没有被重写，它默认继承自 `Object` 类，通常返回的是对象的内存地址相关的整数。这会导致即使两个对象的内容（根据业务逻辑判断是相等的）相同，只要它们是不同的实例，`hashCode()` 就可能不同，从而被映射到不同的桶，这违反了 Map 对于“相同 Key”的定义。
2. **`equals()`**\*\* 用于精确匹配:\*\* 当发生哈希冲突（多个 Key 映射到同一个桶）时，或者在找到目标桶后需要确认是否是完全相同的 Key 时，`HashMap` 会调用 Key 对象的 `equals()` 方法来进行精确比较。如果 `equals()` 没有被重写，它默认也继承自 `Object` 类，执行的是 `==` 比较，即比较两个引用是否指向同一个内存地址。这同样会导致内容相同但实例不同的对象被认为是不同的 Key。

**`hashCode()`**\*\* 和 ****`equals()`**** 必须遵守的契约：\*\*

- **一致性 (Consistency):** 对于任何非 `null` 的引用值 `x` 和 `y`，如果 `x.equals(y)` 多次调用都返回 `true`，那么 `x.hashCode()` 必须始终等于 `y.hashCode()`。 **这是最核心的约束。**
- **等价性 (Equality implies equal hash codes):** 如果两个对象根据 `equals(Object)` 方法是相等的，那么调用这两个对象中任意一个的 `hashCode` 方法都必须产生同样的整数结果。
- **非等价性 (Unequal objects MAY have equal hash codes):** 如果两个对象根据 `equals(java.lang.Object)` 方法是不相等的，那么调用这两个对象中任一对象的 `hashCode` 方法，不要求必须产生不同的整数结果（即允许哈希冲突）。但是，为不相等的对象产生不同的整数结果，有可能提高哈希表的性能。
- **`equals()`**\*\* 自身的特性:\*\* 需要满足自反性 (`x.equals(x)`), 对称性 (`x.equals(y)` iff `y.equals(x)`), 传递性 (`x.equals(y)` and `y.equals(z)` implies `x.equals(z)`), 以及对 `null` 的一致性 (`x.equals(null)` is false)。

**不遵守契约的后果：**

- 如果只重写 `equals()` 而不重写 `hashCode()`：可能导致两个 `equals()` 相等的对象具有不同的 `hashCode`，它们会被放到不同的桶中。结果是，你用一个对象 `put` 进去，用另一个内容相同但实例不同的对象去 `get`，会返回 `null`，好像这个 Key 从未存在过一样。`HashMap` 实际上可能存储了多个逻辑上“相等”的 Key。
- 如果只重写 `hashCode()` 而不重写 `equals()` (且 `hashCode` 相同)：当发生哈希冲突时，`HashMap` 最终会依赖 `equals()` 来区分。如果 `equals()` 仍然是 `Object` 的默认实现 (比较地址)，那么内容相同但实例不同的对象会被认为是不同的 Key，即使它们的 `hashCode` 相同。

**示例 (伪代码):**

```java
class Person {
    String name;
    int age;

    // Constructor, getters...

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        Person person = (Person) o;
        return age == person.age && Objects.equals(name, person.name);
    }

    @Override
    public int hashCode() {
        // 必须保证 equals() 相等的对象 hashCode() 也相等
        return Objects.hash(name, age); // 使用 Objects.hash() 是推荐的简便方式
    }
}

Map<Person, String> personMap = new HashMap<>();
Person p1 = new Person("Alice", 30);
Person p2 = new Person("Alice", 30); // 内容相同，但实例不同

// 由于正确重写了 equals 和 hashCode:
// p1.equals(p2) -> true
// p1.hashCode() == p2.hashCode() -> true

personMap.put(p1, "Engineer");
String job = personMap.get(p2); // 能够成功获取到 "Engineer"
System.out.println(personMap.size()); // 输出 1
```

**总结:** 使用自定义对象作为 `HashMap` 的 Key 时，**务必同时、正确地重写 ****`equals()`**** 和 ****`hashCode()`**** 方法，并确保它们遵循 Java 规范中的契约**，否则 `HashMap` 将无法按预期工作。此外，还应考虑将 Key 对象设计为 **不可变 (Immutable)** 的，以避免在对象放入 Map 后其状态改变导致 `hashCode` 或 `equals` 结果变化，从而“丢失”在 Map 中的位置。

---

希望这篇详尽的文章能帮助你更好地理解 `HashMap`，并在面试中自信地回答相关问题！祝你面试顺利！🎉

[1. 核心属性](<1. 核心属性/1. 核心属性.md> "1. 核心属性")

[2. 添加数据](<2. 添加数据/2. 添加数据.md> "2. 添加数据")

[3. 扩容机制](<3. 扩容机制/3. 扩容机制.md> "3. 扩容机制")

[4. 寻址算法](<4. 寻址算法/4. 寻址算法.md> "4. 寻址算法")

[5. JDK7多线程死循环](<5. JDK7多线程死循环/5. JDK7多线程死循环.md> "5. JDK7多线程死循环")
