---
title: Java 核心集合类：`java.util.HashMap` 深度解析
date: '2025-04-25'
tags: ['java', 'HashMap']
draft: false
summary: java.util.HashMap 是 Java 集合框架（Java Collections Framework, JCF）中 `Map` 接口最常用的实现类之一。它提供了一种存储键值对（Key-Value Pair）映射关系的数据结构。其核心是基于 **哈希表（Hash Table）** 实现的，这使得它在理想情况下能够提供常数时间复杂度 O(1) 的插入、删除和查找操作。
images: []
---

# 1. HashMap

---

## Java 核心集合类：`java.util.HashMap` 深度解析 🗺️

### 1. 概述与定义 📜

`java.util.HashMap` 是 Java 集合框架（Java Collections Framework, JCF）中 `Map` 接口最常用的实现类之一。它提供了一种存储键值对（Key-Value Pair）映射关系的数据结构。其核心是基于 **哈希表（Hash Table）** 实现的，这使得它在理想情况下能够提供常数时间复杂度 O(1) 的插入、删除和查找操作。

**核心概念定义：**

- **键值对 (Key-Value Pair):** `HashMap` 存储的是成对的数据，每个数据项包含一个唯一的键（Key）和一个对应的值（Value）。通过键可以快速地找到对应的值。
- **基于哈希 (Hash-Based):** `HashMap` 内部通过计算键的哈希码 (`hashCode()`) 来决定其在内部存储结构（通常是一个数组，称为“桶”或“bucket”）中的位置。
- **允许 Null:** `HashMap` 允许 **一个** `null` 键和 **多个** `null` 值。`null` 键总是被映射到哈希表的特定位置（通常是索引 0）。
- **无序性 (Unordered):** `HashMap` 不保证元素的迭代顺序。元素的顺序可能会因为哈希冲突、扩容等原因在任何时候发生变化。你不应该依赖于 `HashMap` 的迭代顺序。如果需要有序的 Map，应考虑 `LinkedHashMap`（按插入或访问顺序）或 `TreeMap`（按自然顺序或自定义比较器顺序）。
- **非线程安全 (Not Thread-Safe):** `HashMap` 的实现不是同步的。如果在多线程环境中并发地修改（添加、删除元素，或者结构性修改如扩容）`HashMap`，并且没有外部同步措施，可能会导致数据不一致，甚至在 JDK 7 及更早版本中可能因扩容操作引发死循环。

`HashMap` 是 Java 开发中用于快速查找和关联数据的基石，理解其内部工作原理对于编写高效且健壮的代码至关重要。

### 2. 主要特点 ✨

`HashMap` 之所以被广泛使用，主要归功于其以下显著特点：

1. **高效的性能** 🚀:
   - **平均 O(1) 复杂度:** 对于 `put(K key, V value)` 和 `get(Object key)` 操作，在哈希分布均匀的情况下，平均时间复杂度为 O(1)。这是其最核心的优势。
   - **最坏 O(n) 复杂度:** 在极端情况（例如，所有键的哈希码都相同，导致所有元素集中在一个桶中形成长链表或复杂的树结构），时间复杂度会退化到 O(n)，其中 n 是 Map 中的元素数量。JDK 8 引入红黑树优化了这种情况下的性能，使其最坏复杂度降为 O(log n)。
2. **依赖 ****`hashCode()`**** 和 **`equals()`** 🔑🤝:
   - `HashMap` 的正确运行严重依赖于作为键的对象的 `hashCode()` 和 `equals()` 方法的正确实现。
   - `hashCode()` 用于快速定位桶的位置。
   - `equals()` 用于在发生哈希冲突时（即多个不同的键映射到同一个桶），精确地识别和区分键。
   - **重要契约:**
     - 如果 `a.equals(b)` 为 `true`，那么 `a.hashCode()` 必须等于 `b.hashCode()`。
     - 如果 `a.equals(b)` 为 `false`，`a.hashCode()` 和 `b.hashCode()` 可以相同（哈希冲突），也可以不同。
     - 一个对象的 `hashCode()` 值在其生命周期内，只要其 `equals()` 方法的比较信息没有改变，就应该保持不变。
   - 不遵守这个契约会导致 `HashMap` 无法正常工作（例如，`put` 进去的对象 `get` 不出来）。
3. **动态扩容 (Resizing)** 📈:
   - 当 `HashMap` 中的元素数量超过 **阈值 (threshold = capacity \* load factor)** 时，会自动进行扩容（通常是将容量翻倍），并将所有现有元素重新计算哈希位置（rehash）到新的、更大的数组中。这有助于维持较低的哈希冲突率，保证性能。
4. **空间换时间** 💰\<-\>⏱️:
   - 哈希表是一种典型的空间换时间的策略。它使用额外的内存空间（内部数组）来加速查找过程。
5. **灵活性 (Null Support):** 如前所述，支持 `null` 键和 `null` 值，提供了使用的便利性，但在某些场景下也可能引入 `NullPointerException` 的风险，需要开发者注意。

### 3. 应用目标 (Use Cases) 🎯

`HashMap` 的设计目标是提供一个高效的、通用的键值对存储机制。它适用于以下典型场景：

1. **数据缓存 (Caching):** 快速存储和检索频繁访问的数据，如用户信息、配置项等。内存缓存是 `HashMap` 非常常见的应用。

   ```java
   // 简单内存缓存示例
   Map<String, UserData> userCache = new HashMap<>();
   UserData user = getUserFromDatabase("userId123");
   userCache.put("userId123", user);
   // ... later
   UserData cachedUser = userCache.get("userId123");
   if (cachedUser != null) {
       // Use cached data
   } else {
       // Fetch from database and cache
   }
   ```
2. **配置管理 (Configuration Management):** 存储和查找应用程序的配置参数。

   ```java
   Map<String, String> appConfig = new HashMap<>();
   appConfig.put("database.url", "jdbc:mysql://localhost:3306/mydb");
   appConfig.put("service.timeout", "5000");
   String dbUrl = appConfig.get("database.url");
   ```
3. **频率统计 (Frequency Counting):** 统计集合中各元素出现的次数。

   ```java
   String text = "hello world hello java";
   Map<String, Integer> wordCounts = new HashMap<>();
   for (String word : text.split(" ")) {
       wordCounts.put(word, wordCounts.getOrDefault(word, 0) + 1);
   }
   // wordCounts will be: {"hello": 2, "world": 1, "java": 1}
   ```
4. **数据索引 (Indexing):** 根据某个属性（作为 Key）快速查找对象（作为 Value）。

   ```java
   List<Product> productList = getAllProducts();
   Map<String, Product> productIndexById = new HashMap<>();
   for (Product p : productList) {
       productIndexById.put(p.getId(), p);
   }
   Product specificProduct = productIndexById.get("prod789");
   ```
5. **传递复杂参数:** 在方法间或模块间传递一组命名的参数。

总之，任何需要通过一个唯一的标识符（键）来快速存储、查找、更新或删除关联信息（值）的场景，`HashMap` 都是首选的数据结构之一。

### 4. 主要内容及其组成部分 (Internal Structure) 🏗️

理解 `HashMap` 的内部结构是掌握其工作原理的关键。其核心组成部分包括（以 JDK 8+ 为例）：

1. **桶数组** (Bucket Array): **`transient Node<K,V>[] table;`**

   - 这是 `HashMap` 的主体结构，一个 `Node` 类型的数组。每个数组元素称为一个“桶”（Bucket）。
   - 数组的长度（Capacity）**总是 2 的幂次方** (e.g., 16, 32, 64...)。这是为了使用高效的位运算 `(n - 1) & hash` 来代替取模运算 `%` 计算索引，提高性能。
   - 数组是 `transient` 的，意味着它不会被默认的序列化机制序列化。`HashMap` 有自定义的 `writeObject` 和 `readObject` 方法来处理序列化。
2. **节点 (Node):** **`static class Node<K,V> implements Map.Entry<K,V>`**

   - 这是存储键值对的基本单元，实现了 `Map.Entry` 接口。
   - 每个 `Node` 对象包含：
     - `final int hash;`: 键的哈希值（经过 `HashMap` 内部 `hash()` 方法处理）。存储哈希值是为了在查找和扩容时避免重复计算。
     - `final K key;`: 键对象。
     - `V value;`: 值对象。
     - `Node<K,V> next;`: 指向同一个桶中的下一个节点的引用。这用于解决哈希冲突，形成 **链表**。
3. **红黑树节点 (TreeNode)** **`static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V>`** (JDK 8+)

   - 当某个桶中的链表长度达到一定阈值 (`TREEIFY_THRESHOLD`)，并且 `HashMap` 的总容量也达到一定阈值 (`MIN_TREEIFY_CAPACITY`) 时，这个链表会被转换成 **红黑树 (Red-Black Tree)** 结构。
   - `TreeNode` 继承自 `LinkedHashMap.Entry`（它本身继承自 `HashMap.Node`），并添加了红黑树所需的额外属性，如 `parent`, `left`, `right`, `red` (颜色标记)。
   - 使用红黑树的目的是将该桶内的查找、插入、删除操作的时间复杂度从 O(n) 优化到 O(log n)，提高在哈希冲突严重时的性能。
4. **重要常量与变量:**

   - **`size`**: `HashMap` 中实际存储的键值对数量。
   - **`modCount`**: 修改计数器。用于迭代器实现快速失败 (Fail-Fast) 机制。每次结构性修改（增、删、扩容）时，`modCount` 都会增加。迭代器在迭代过程中会检查 `modCount` 是否与创建迭代器时的值一致，不一致则抛出 `ConcurrentModificationException`。
   - **`threshold`**: 扩容阈值。当 `size` 超过 `threshold` 时，触发 `resize()` 操作。`threshold = capacity * loadFactor`。
   - **`loadFactor`**: 负载因子。表示哈希表的填充程度。
     **核心常量解释表:**
     |名                  | 默认值 | 含义                                                                                                                                 |
     | :---------------------- | :----- | :----------------------------------------------------------------------------------------------------------------------------------- |
     | `DEFAULT_INITIAL_CAPACITY` | 16     | 默认初始容量（桶数组大小）。若未指定，则首次 `put` 时创建大小为 16 的数组。                                                               |
     | `MAXIMUM_CAPACITY`      | 2^30   | 最大容量。即使指定或计算出的容量超过此值，也会被限制为此值。                                                                              |
     | `DEFAULT_LOAD_FACTOR`   | 0.75f  | 默认负载因子。这是时间和空间成本之间的一个平衡选择。值越小，冲突越少，但空间利用率低；值越大，空间利用率高，但冲突增多，性能可能下降。 |
     | `TREEIFY_THRESHOLD`     | 8      | 链表转红黑树的阈值。当一个桶中的节点数达到此值时，**可能**会触发树化。                                                                    |
     | `UNTREEIFY_THRESHOLD`   | 6      | 红黑树转链表的阈值。在 `resize` 操作中，如果树节点数减少到此值，会转回链表结构。                                                           |
     | `MIN_TREEIFY_CAPACITY`  | 64     | 触发树化的最小哈希表容量。即使链表长度达到 `TREEIFY_THRESHOLD`，如果当前 `table` 的容量小于此值，则优先进行扩容 (`resize`) 而不是树化。       |

   **HashMap 内部结构示意图 (Mermaid):**

   ```mermaid
   graph TD
       subgraph MainMap [HashMap Structure]
           A[Array] --> B(B0);
           A --> C(B1);
           A --> D(...);
           A --> E(Bi);
           A --> F(Bj);
           A --> G(Bn_1);

           subgraph ListBucket [Bucket i: List]
               N1 --> N2 --> N3;
           end
           E --> N1;

           subgraph TreeBucket [Bucket j: Tree]
               T1 --> T2;
               T1 --> T3;
               T2 --> T4;
           end
           F --> T1;

           C --> N4;
           G --> N5;
           B --- NullNode;
       end
   ```

   **图表解释:** 上图展示了 `HashMap` 内部的 `table` 数组（桶数组）。每个桶可能为空，或者包含一个 `Node`。如果发生哈希冲突，多个 `Node` 会通过 `next` 指针形成链表（如 Bucket i）。当链表过长（且满足条件）时，会转换为红黑树结构（如 Bucket j）。`null` 键通常存储在 Bucket 0。

### 5. 原理剖析 (深入解析, 源码级) ⚙️🔬

理解 `HashMap` 的核心操作原理是面试中的重中之重。

#### **哈希函数 (****`hash(Object key)`****)**

`HashMap` 并不是直接使用键的 `hashCode()` 返回值。它内部有一个 `hash()` 方法对键的 `hashCode()` 进行再处理：

```java
static final int hash(Object key) {
    int h;
    // 如果 key 是 null，返回 0。
    // 否则，获取 key 的 hashCode()，然后将 hashCode 的高 16 位与低 16 位进行异或 (^) 运算。
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

**目的：** 这样做是为了**扰动计算**，让哈希值的高位也能参与到最终索引的计算中，减少哈希冲突的可能性。因为计算索引时通常只用了低位（` (n - 1) & hash` ），如果不进行扰动，即使 `hashCode()` 实现得很好，如果高位变化很大而低位变化小，也容易造成冲突。

#### **计算索引 (****`indexFor(hash, length)`**** 或 ****`(n - 1) & hash`****)**

确定键值对在 `table` 数组中的位置（索引）使用位运算：

```java
// n 是 table 的长度 (capacity)，必须是 2 的幂次方
int index = (n - 1) & hash;
```

**原理：** 当 `n` 是 2 的幂次方时，`n - 1` 的二进制表示就是一串连续的 1（例如，`n=16`, `n-1=15`, 二进制 `1111`）。`hash & (n - 1)` 的效果等同于 `hash % n`，但是位运算 `&` 比取模运算 `%` 效率高得多。这就是为什么 `HashMap` 容量强制为 2 的幂次方。

#### **`put(K key, V value)`** **方法详解**

`put` 操作是 `HashMap` 最核心的操作之一，其流程（JDK 8+）大致如下：

1. **判空与哈希:**
   - 如果 `table` 未初始化或长度为 0，调用 `resize()` 进行初始化（通常是默认容量 16）。
   - 计算键 `key` 的哈希值 `hash = hash(key)`。
   - 计算索引 `i = (table.length - 1) & hash`。
2. **检查桶位:**
   - 获取 `table[i]` 处的节点 `p`。
   - **如果 ****`p`**** 为 ****`null`****:** 说明该桶为空，直接创建一个新的 `Node(hash, key, value, null)` 放入 `table[i]`。
3. **处理冲突 (桶位非空):**
   - **如果 ****`p`**** 的哈希值等于 ****`hash`**** 且 ****`p`**** 的键等于 ****`key`**** (通过 ****`equals()`**** 判断):** 说明找到了相同的键，用新值 `value` 覆盖旧值。将旧值存储在 `e` 中待返回。
   - **如果 ****`p`**** 是 ****`TreeNode`****:** 调用红黑树的插入方法 `p.putTreeVal(this, table, hash, key, value)`。树结构内部会处理键重复或插入新节点的情况。
   - **如果 ****`p`**** 是普通 ****`Node`**** (链表头):** 遍历链表：
     - 使用 `for` 循环和计数器 `binCount` 遍历链表节点。
     - 对于每个节点 `e`，检查其 `hash` 和 `key` 是否与待插入的 `key` 相同。如果相同，则找到重复键，用新 `value` 替换旧 `value`，记录旧值，并跳出循环。
     - 如果遍历到链表末尾（`e.next == null`）仍未找到相同键，则在链表尾部插入新的 `Node(hash, key, value, null)`。
     - **树化检查:** 在插入新节点**之后**，检查链表长度 `binCount` 是否达到 `TREEIFY_THRESHOLD - 1`（即插入后长度为 8）。如果是，并且 `table.length >= MIN_TREEIFY_CAPACITY`，则调用 `treeifyBin(table, hash)` 将此链表转换为红黑树。
     - 如果找到重复键并更新了值，则跳出链表遍历。
4. **收尾工作:**
   - 如果 `put` 操作确实插入了一个新的键值对（而不是更新现有键的值），则：
     - 递增 `modCount`。
     - 递增 `size`。
     - 检查 `size` 是否超过 `threshold`。如果是，则调用 `resize()` 进行扩容。
   - `afterNodeInsertion(evict)`: 这是一个空方法，留给子类（如 `LinkedHashMap`）实现插入后逻辑。
   - 返回旧值（如果发生替换）或 `null`（如果是新插入）。

**`put`** **流程图 (Mermaid - Simplified):**

```mermaid
graph TD
    A["Start put(key, value)"] --> B{"table null?"};
    %% Using quotes for node text with special chars/spaces is safer
    B -- Yes --> C["resize() to initialize"];
    B -- No --> D["Calculate hash & index i"];
    C --> D;
    D --> E{"table[i] == null?"};
    E -- Yes --> F["Create Node, put in table[i]"];
    E -- No --> G{"First node matches key?"};
    G -- Yes --> H["Update value, store old"];
    G -- No --> I{"First node TreeNode?"};
    I -- Yes --> J["Call putTreeVal()"];
    I -- No --> K["Traverse LinkedList"];
    K --> L{"Found matching key?"};
    L -- Yes --> H;
    L -- No --> M["Append new Node to list tail"];
    M --> N{"List length >= TREEIFY_THRESHOLD - 1?"};
    N -- Yes --> O{"table.length >= MIN_TREEIFY_CAPACITY?"};
    O -- Yes --> P["treeifyBin()"];
    %% Link from O when condition is No
    O -- No --> Q;
    %% Link from N when condition is No
    N -- No --> Q;
    P --> Q;
    J --> Q;
    %% Link from H to R
    H --> R;
    F --> Q["Increment size, modCount"];
    Q --> S{"size > threshold?"};
    S -- Yes --> T["resize()"];
    S -- No --> R["Return old value or null"];
    T --> R;
```

**图表解释:** 该流程图简化了 `put` 方法的主要逻辑：初始化检查、计算位置、空桶处理、冲突处理（检查首节点、树节点、遍历链表）、插入新节点、可能的树化、更新计数器、检查扩容，最后返回。

#### **`get(Object key)`** **方法详解**

`get` 操作相对简单：

1. **计算哈希与索引:** 计算 `key` 的 `hash` 和 `table` 中的索引 `i`。
2. **查找桶:** 获取 `table[i]` 处的节点 `first`。
3. **检查首节点:** 如果 `first` 不为 `null`：
   - 检查 `first.hash` 是否等于 `hash` 并且 `first.key` 是否等于 `key` (通过 `equals()` 判断)。如果匹配，返回 `first.value`。
4. **查找后续节点:** 如果首节点不匹配且 `first.next` 不为 `null`：
   - **如果是 ****`TreeNode`****:** 调用 `first.getTreeNode(hash, key)` 在红黑树中查找。
   - **如果是链表:** 遍历链表（`do...while` 循环），对每个节点 `e`，检查 `hash` 和 `key` 是否匹配。找到则返回 `e.value`。
5. **未找到:** 如果遍历完桶（或桶为空）仍未找到匹配的键，返回 `null`。

#### **`resize()`** **方法详解**

扩容是 `HashMap` 维护性能的关键，也是相对复杂的操作（尤其在 JDK 8+ 中有优化）：

1. **容量计算:**
   - 获取旧容量 `oldCap` 和旧阈值 `oldThr`。
   - 计算新容量 `newCap` (通常是 `oldCap << 1`，即翻倍) 和新阈值 `newThr` (通常是 `newCap * loadFactor`)。需要处理边界情况（如初始容量、超过最大容量）。
   - 如果 `oldCap` 已经达到 `MAXIMUM_CAPACITY`，则不再扩容，只调整 `threshold` 为 `Integer.MAX_VALUE`。
2. **创建新表:** 创建一个新的 `Node` 数组 `newTab`，长度为 `newCap`。
3. **迁移元素:** 遍历旧表 `oldTab` 的每一个桶（`for` 循环 `j` from 0 to `oldCap`）：
   - 获取旧桶的头节点 `e = oldTab[j]`。
   - 如果 `e` 不为 `null`：
     - 将 `oldTab[j]` 设为 `null`（帮助 GC）。
     - **如果 ****`e.next == null`****:** 该桶只有一个节点。直接计算它在新表中的索引 `(newCap - 1) & e.hash`，并放入 `newTab`。
     - **如果 ****`e`**** 是 ****`TreeNode`****:** 调用 `((TreeNode<K,V>)e).split(this, newTab, j, oldCap)`。这个方法利用红黑树的性质，高效地将树拆分成两个子树或链表，分别放到新表中的 `j` 和 `j + oldCap` 位置。
     - **如果是链表 (JDK 8+ 优化):**
       - 维护两个链表的头尾指针：`loHead`, `loTail` (用于索引 `j`) 和 `hiHead`, `hiTail` (用于索引 `j + oldCap`)。
       - 遍历旧链表中的每个节点 `e`：
         - **判断依据:** `(e.hash & oldCap) == 0`。这个位运算巧妙地利用了新容量是旧容量两倍的特性。如果结果为 0，节点在新表中的索引仍然是 `j`；如果结果非 0，索引将是 `j + oldCap`。
         - 根据判断结果，将节点追加到 `lo` 链表或 `hi` 链表的尾部。
       - 遍历结束后，如果 `loHead` 不为空，将 `lo` 链表放入 `newTab[j]`。
       - 如果 `hiHead` 不为空，将 `hi` 链表放入 `newTab[j + oldCap]`。
       - 这个优化避免了对链表中每个节点重新计算完整的哈希索引，只需一次位运算即可确定其在新表中的两个可能位置之一。
4. **更新引用:** 将 `HashMap` 的 `table` 字段指向 `newTab`，更新 `threshold`。

**`resize()`** **中 JDK 8 的链表迁移优化示意图 (Mermaid):**

```mermaid
graph TD
    %% Subgraph for the old table
    subgraph Old_Table [Old Table Capacity 16]
        Bucket_j[Bucket j] --> N1((Node 1));
        N1 --> N2((Node 2));
        N2 --> N3((Node 3));
        N3 --> N4((Node 4));
    end

    %% Subgraph for the new table
    subgraph New_Table [New Table Capacity 32]
        NewBucket_j["Bucket j"];
        NewBucket_j_plus_oldCap["Bucket j + 16"];
    end

    %% Subgraph showing the processing logic for one bucket
    subgraph Processing_Bucket_j [Processing Bucket j]
        %% Start processing the first node
        P1{"Loop: Node 1 (e)"};
        %% Check the hash bit to decide low or high list
        P1 -- "(e.hash & 16) == 0" --> AddToLowList["Add N1 to Low List index j"];
        P1 -- "(e.hash & 16) != 0" --> AddToHighList["Add N1 to High List index j+16"];

        %% Process the second node
        P2{"Loop: Node 2 (e)"};
        P2 -- "(e.hash & 16) == 0" --> AddToLowList;
        P2 -- "(e.hash & 16) != 0" --> AddToHighList;

        %% Process the third node
        P3{"Loop: Node 3 (e)"};
        P3 -- "(e.hash & 16) == 0" --> AddToLowList;
        P3 -- "(e.hash & 16) != 0" --> AddToHighList;

        %% Process the fourth node
        P4{"Loop: Node 4 (e)"};
        P4 -- "(e.hash & 16) == 0" --> AddToLowList;
        P4 -- "(e.hash & 16) != 0" --> AddToHighList;
    end

    %% Represent the resulting low and high lists
    AddToLowList --> FinalLowList["Low List Head --> ... --> Low List Tail"];
    AddToHighList --> FinalHighList["High List Head --> ... --> High List Tail"];

    %% Show placement of lists into the new table
    FinalLowList --> PlaceLow["Place Low List in NewBucket_j"];
    FinalHighList --> PlaceHigh["Place High List in NewBucket_j_plus_oldCap"];
    PlaceLow --> NewBucket_j;
    PlaceHigh --> NewBucket_j_plus_oldCap;

    %% Dotted links indicating iteration/processing flow over original list
    Bucket_j -.-> P1;
    N1 -.-> P2;
    N2 -.-> P3;
    N3 -.-> P4;

```

**图表解释:** 当从容量 16 扩容到 32 时，旧 Bucket `j` 中的链表节点会被遍历。通过检查 `(e.hash & 16)`（`16` 是旧容量 `oldCap`）是否为 0，将节点分配到两个新链表：Low List（保持在索引 `j`）和 High List（移动到索引 `j + 16`）。最后将这两个新链表分别放入新表对应位置。

#### **JDK 7 vs JDK 8+ 的主要差异**

- **冲突解决:** JDK 7 只使用链表。JDK 8+ 在链表长度达到阈值时会转换为红黑树。
- **`put`**** 插入:** JDK 7 使用头插法，JDK 8+ 使用尾插法。头插法在并发 `resize` 时可能导致链表成环，引发死循环。尾插法避免了这个问题。
- **`resize`**** 迁移:** JDK 7 需要重新计算每个元素在新表中的完整索引。JDK 8+ 对链表迁移做了优化（如上图所示），效率更高。

### 6. 应用与拓展 🚀

#### **最佳实践与注意事项**

1. **预估容量:** 如果能预估 `HashMap` 将存储的元素数量，在创建时指定 `initialCapacity` 可以避免或减少后续的 `resize` 操作，提高性能。计算合适的 `initialCapacity` 时，考虑负载因子：`initialCapacity = (int) (expectedSize / loadFactor) + 1`。
2. **自定义对象作 Key:** 如果使用自定义类的对象作为 `HashMap` 的键，**必须**正确地重写 `hashCode()` 和 `equals()` 方法，并保证它们遵守约定。
3. **避免使用可变对象作 Key:** 如果一个对象作为 Key 放入 `HashMap` 后，其内部状态发生改变，导致 `hashCode()` 值变化或 `equals()` 比较结果变化，那么可能无法再通过 `get()` 方法取回该对象。通常建议使用不可变对象（如 `String`, `Integer`, 或自定义的不可变类）作为 Key。
4. **理解 ****`null`****:** 注意 `HashMap` 对 `null` 键和值的支持，在处理时考虑 `NullPointerException` 的可能性。

#### **线程安全问题与替代方案**

- **`HashMap`** **是非线程安全的。** 在多线程环境下：
  - 并发 `put` 可能导致数据丢失（某个线程的写入被覆盖）。
  - 并发 `put` 和 `get` 可能读到不一致的数据。
  - 并发 `resize` (尤其 JDK 7) 可能导致链表循环，使得 `get` 操作陷入死循环，CPU 飙升。
  - 在迭代过程中进行结构性修改（非迭代器自身的 `remove`），会触发 `ConcurrentModificationException` (Fail-Fast)。
- **线程安全的替代方案:**
  - **`Hashtable`:** 古老的线程安全 Map 实现，所有公开方法都使用 `synchronized` 修饰，锁粒度是整个对象。性能较差，因为它在任何操作时都会锁住整个表，并发度低。不允许 `null` 键和值。
  - **`Collections.synchronizedMap(Map<K,V> m)`:** 一个包装器方法，返回指定 Map 的线程安全视图。其内部也是通过 `synchronized` 锁住整个 Map 对象来实现同步。性能与 `Hashtable` 类似，并发度低。
  - **`ConcurrentHashMap`:** (推荐 👍) JUC 包下的类，专为高并发场景设计。它采用了更细粒度的锁机制（JDK 7 的分段锁 Segment，JDK 8+ 的 CAS + `synchronized` 锁住 Node 或 ForwardingNode），允许多个线程同时读写 Map 的不同部分，提供了高得多的并发性能。它是现代 Java 并发编程中替代 `HashMap` 和 `Hashtable` 的首选。

#### **有序 Map**

如果需要保持 Map 中元素的顺序，应使用：

- **`LinkedHashMap`:** 继承自 `HashMap`，额外维护了一个双向链表来记录元素的插入顺序或访问顺序。迭代时按此顺序返回。性能略低于 `HashMap`（因为维护链表的开销），但仍然是 O(1) 级别的。
- **`TreeMap`:** 基于红黑树实现。元素根据键的自然顺序（需要 Key 实现 `Comparable` 接口）或构造时提供的 `Comparator` 进行排序。提供了按排序顺序迭代的功能，以及一些基于排序的操作（如 `firstKey()`, `lastKey()`, `subMap()` 等）。查找、插入、删除操作的平均和最坏时间复杂度都是 O(log n)。



